{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2612,"status":"ok","timestamp":1640501808260,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"LuCjrpiv0KE1","outputId":"b2644785-82fd-46d7-ff94-c9a6b5ac3f65"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49094,"status":"ok","timestamp":1640499916742,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"3D9HztSq0FTG","outputId":"f1cef18a-b617-4e19-cca5-47def2a263b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading roberta-base.zip to /content\n","100% 1.31G/1.31G [00:10<00:00, 144MB/s]\n","100% 1.31G/1.31G [00:10<00:00, 135MB/s]\n","Archive:  roberta-base.zip\n","  inflating: input/roberta-base/README.md  \n","  inflating: input/roberta-base/config.json  \n","  inflating: input/roberta-base/dict.txt  \n","  inflating: input/roberta-base/flax_model.msgpack  \n","  inflating: input/roberta-base/merges.txt  \n","  inflating: input/roberta-base/pytorch_model.bin  \n","  inflating: input/roberta-base/rust_model.ot  \n","  inflating: input/roberta-base/tf_model.h5  \n","  inflating: input/roberta-base/tokenizer.json  \n","  inflating: input/roberta-base/vocab.json  \n","Downloading jigsaw-toxic-severity-rating.zip to /content\n"," 74% 5.00M/6.72M [00:00<00:00, 38.3MB/s]\n","100% 6.72M/6.72M [00:00<00:00, 49.5MB/s]\n","Archive:  jigsaw-toxic-severity-rating.zip\n","  inflating: input/jigsaw-toxic-severity-rating/comments_to_score.csv  \n","  inflating: input/jigsaw-toxic-severity-rating/sample_submission.csv  \n","  inflating: input/jigsaw-toxic-severity-rating/validation_data.csv  \n","Downloading jigsaw-folds.zip to /content\n"," 45% 5.00M/11.0M [00:00<00:00, 43.5MB/s]\n","100% 11.0M/11.0M [00:00<00:00, 68.8MB/s]\n","Archive:  jigsaw-folds.zip\n","  inflating: input/jigsaw-folds/train_10Folds.csv  \n","  inflating: input/jigsaw-folds/train_5folds.csv  \n","CPU times: user 456 ms, sys: 159 ms, total: 614 ms\n","Wall time: 47.4 s\n"]}],"source":["%%time\n","! pip install --upgrade --force-reinstall --no-deps  kaggle > /dev/null\n","! mkdir ~/.kaggle\n","\n","! cp \"/content/drive/My Drive/Kaggle/kaggle.json\" ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","\n","import os\n","\n","if not os.path.exists(\"/content/input/\"):\n","    !mkdir input\n","    !kaggle datasets download -d abhishek/roberta-base\n","    !unzip roberta-base.zip -d input/roberta-base\n","\n","    !kaggle competitions download -c jigsaw-toxic-severity-rating\n","    !unzip jigsaw-toxic-severity-rating.zip -d input/jigsaw-toxic-severity-rating\n","\n","    !kaggle datasets download -d ishandutta/jigsaw-folds\n","    !unzip jigsaw-folds.zip -d input/jigsaw-folds"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32594,"status":"ok","timestamp":1640499955087,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"jtIASeGdyIQb","outputId":"dddbb5c9-9f15-4b02-9357-858c4b2e11ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wandb\n","  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 7.5 MB/s \n","\u001b[?25hCollecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.5.1-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 53.4 MB/s \n","\u001b[?25hCollecting yaspin>=1.0.0\n","  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Collecting subprocess32>=3.5.3\n","  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 6.5 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting configparser>=3.8.1\n","  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 57.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n","Building wheels for collected packages: subprocess32, pathtools\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=3b2c8741341f69f17195581af3618a0f9af9a7a0af5a82ed7635f5f5dd7dff46\n","  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=2cc7d00d20832c2753ff5f4765991a1d36925ad23eaf5a927f4f62bc946b72b0\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built subprocess32 pathtools\n","Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n","Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.1 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.9 yaspin-2.1.0\n","Collecting transformers\n","  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 8.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 48.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 42.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 442 kB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 53.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n","Collecting pytorch_lightning\n","  Downloading pytorch_lightning-1.5.7-py3-none-any.whl (526 kB)\n","\u001b[K     |████████████████████████████████| 526 kB 8.0 MB/s \n","\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 52.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n","\u001b[K     |████████████████████████████████| 332 kB 62.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n","Collecting future>=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 43.6 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n","Collecting pyDeprecate==0.3.1\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 17.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.42.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 51.8 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 65.3 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.8)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 51.9 MB/s \n","\u001b[?25hBuilding wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=cdec13c6a52a250c258bb8d31bb0f970466de62dbbd9e03d8c79d650933d804e\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","Successfully built future\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, pyDeprecate, future, pytorch-lightning\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.7 torchmetrics-0.6.2 yarl-1.7.2\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Installing collected packages: colorama\n","Successfully installed colorama-0.4.4\n"]}],"source":["!pip install wandb\n","!pip install transformers\n","!pip install pytorch_lightning\n","!pip install colorama"]},{"cell_type":"markdown","metadata":{"id":"VwI2x9A1xjTV"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"slQPV5_ExjTV"},"source":["---"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1640501816144,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"6bGfdIJZLav9","outputId":"4a703038-ef1b-4bc1-b8cf-4a19b3efe2b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/python/kaggle/jigsaw_study_lightning/code\n"]}],"source":["cd /content/drive/MyDrive/python/kaggle/jigsaw_study_lightning/code/"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10299,"status":"ok","timestamp":1640501829511,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"eOQ3hHkZ7rMO","outputId":"c25165f6-4bfa-416a-a74c-832329647834"},"outputs":[{"name":"stdout","output_type":"stream","text":["config file ---->  config/config.yaml\n"]}],"source":["# Necessities\n","import wandb\n","import pandas as pd\n","\n","import datetime\n","import os\n","\n","from tqdm import tqdm\n","import numpy as np\n","import gc\n","\n","# PyTorch\n","import torch\n","import torch.nn as nn\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Transformers\n","from transformers import AutoTokenizer, AutoModel, AdamW\n","\n","# PyTorch Lightning\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","\n","# Colored Terminal Text\n","from colorama import Fore, Back, Style\n","b_ = Fore.BLUE\n","y_ = Fore.YELLOW\n","sr_ = Style.RESET_ALL\n","\n","import pathlib\n","from glob import glob\n","import yaml\n","p_temp = pathlib.Path('.')\n","### config yamlの指定はここ。\n","yaml_path = list(p_temp.glob('**/*.yaml'))[0]\n","print(\"config file ----> \", yaml_path)\n","\n","with open(yaml_path, 'r') as yml:\n","    CONFIG = yaml.safe_load(yml)\n","CONFIG = CONFIG['train_args']\n","CONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG[\"tokenizer\"])"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1640501829511,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"mbJt5N-HiU7m"},"outputs":[],"source":["class JigsawModel(pl.LightningModule):\n","    \n","    def __init__(self, model_name):\n","        super(JigsawModel, self).__init__()\n","        self.model = AutoModel.from_pretrained(model_name)\n","        self.drop = nn.Dropout(p=0.2)\n","        self.fc = nn.Linear(768, CONFIG['num_classes'])\n","        \n","    def forward(self, ids, mask):        \n","        out = self.model(input_ids=ids,attention_mask=mask,\n","                         output_hidden_states=False)\n","        out = self.drop(out[1])\n","        outputs = self.fc(out)\n","                    \n","        return outputs\n","    \n","    def training_step(self, batch, batch_idx):\n","        more_toxic_ids = batch['more_toxic_ids']\n","        more_toxic_mask = batch['more_toxic_mask']\n","        less_toxic_ids = batch['less_toxic_ids']\n","        less_toxic_mask = batch['less_toxic_mask']\n","        targets = batch['target']\n","        \n","        more_toxic_outputs = self(more_toxic_ids, more_toxic_mask)\n","        less_toxic_outputs = self(less_toxic_ids, less_toxic_mask)\n","        \n","        loss = self.criterion(more_toxic_outputs, less_toxic_outputs, targets)\n","        \n","        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n","        \n","        return {\"loss\": loss}\n","    \n","    def validation_step(self, batch, batch_idx):\n","        more_toxic_ids = batch['more_toxic_ids']\n","        more_toxic_mask = batch['more_toxic_mask']\n","        less_toxic_ids = batch['less_toxic_ids']\n","        less_toxic_mask = batch['less_toxic_mask']\n","        targets = batch['target']\n","        \n","        more_toxic_outputs = self(more_toxic_ids, more_toxic_mask)\n","        less_toxic_outputs = self(less_toxic_ids, less_toxic_mask)\n","        \n","        loss = self.criterion(more_toxic_outputs, less_toxic_outputs, targets)\n","        \n","        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n","        \n","        return {'val_loss': loss}      \n","        \n","    def configure_optimizers(self):\n","        \n","        optimizer = AdamW(self.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n","        scheduler = fetch_scheduler(optimizer)\n","        \n","        return dict(\n","            optimizer = optimizer,\n","            lr_scheduler = scheduler\n","        )\n","    \n","    def criterion(self, outputs1, outputs2, targets):\n","        return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)"]},{"cell_type":"markdown","metadata":{"id":"WYLxYXAcz8pU"},"source":["## inferernce"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":898,"status":"ok","timestamp":1640500277322,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"5V0a3bN5z--Q"},"outputs":[],"source":["test = pd.read_csv(\"/content/input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n","sample_submission = pd.read_csv(\"/content/input/jigsaw-toxic-severity-rating/sample_submission.csv\")"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":651,"status":"ok","timestamp":1640500285294,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"pn_aKI8X03MW"},"outputs":[],"source":["class JigsawDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_length):\n","        self.df = df\n","        self.max_len = max_length\n","        self.tokenizer = tokenizer\n","        self.text = df['text'].values\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        text = self.text[index]\n","        inputs = self.tokenizer.encode_plus(\n","                        text,\n","                        truncation=True,\n","                        add_special_tokens=True,\n","                        max_length=self.max_len,\n","                        padding='max_length'\n","                    )\n","        \n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']        \n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long)\n","        }"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":260,"status":"ok","timestamp":1640500294314,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"VMKPGfsr06Nt"},"outputs":[],"source":["test_dataset = JigsawDataset(test, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n","test_loader = DataLoader(test_dataset, batch_size=CONFIG['train_batch_size'],\n","                         num_workers=2, shuffle=False, pin_memory=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":972,"status":"ok","timestamp":1640501834958,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"iq_AfCaV4F4a"},"outputs":[],"source":["@torch.no_grad()\n","def valid_fn(model, dataloader, device):\n","    model.eval()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    \n","    PREDS = []\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, data in bar:\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        \n","        outputs = model(ids, mask)\n","        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n","    \n","    PREDS = np.concatenate(PREDS)\n","    gc.collect()\n","    \n","    return PREDS"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236614,"status":"ok","timestamp":1640501154177,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"PE5tZc0T0UjF","outputId":"88833f16-230e-4990-c5ee-f01727296f6d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":[" @@@@@@@@@@@@@@@@@@ fold0 @@@@@@@@@@@@@@@@@@\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 236/236 [00:30<00:00,  7.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":[" @@@@@@@@@@@@@@@@@@ fold1 @@@@@@@@@@@@@@@@@@\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 236/236 [00:30<00:00,  7.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":[" @@@@@@@@@@@@@@@@@@ fold2 @@@@@@@@@@@@@@@@@@\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 3\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 236/236 [00:30<00:00,  7.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":[" @@@@@@@@@@@@@@@@@@ fold3 @@@@@@@@@@@@@@@@@@\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 4\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 236/236 [00:30<00:00,  7.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":[" @@@@@@@@@@@@@@@@@@ fold4 @@@@@@@@@@@@@@@@@@\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 236/236 [00:30<00:00,  7.73it/s]\n"]}],"source":["final_preds = []\n","jigsawamodel = JigsawModel(CONFIG['model_name']) \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","checkpoint = glob(\"../models/roberta-base/**/*\")\n","model_paths = [x for x in checkpoint if CONFIG[\"exp_name\"] in x]\n","\n","for i, path in enumerate(model_paths):\n","    print(f\" @@@@@@@@@@@@@@@@@@ fold{i} @@@@@@@@@@@@@@@@@@\")\n","    model = jigsawamodel.load_from_checkpoint(checkpoint_path=path, model_name=CONFIG['model_name'])\n","    model.to(device)\n","\n","    print(f\"Getting predictions for model {i+1}\")\n","    preds = valid_fn(model, test_loader, device)\n","    final_preds.append(preds)\n","\n","final_preds = np.array(final_preds)\n","final_preds = np.mean(final_preds, axis=0)"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":290,"status":"ok","timestamp":1640501159835,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"oAxfkaRt2Upp"},"outputs":[],"source":["sample_submission['score'] = final_preds\n","sample_submission['score'] = sample_submission['score'].rank(method='first')"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":326,"status":"ok","timestamp":1640501549584,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"CPJb60Wl2PQa"},"outputs":[],"source":["exp_name = CONFIG[\"exp_name\"]\n","sample_submission.to_csv(f\"../outputs/{exp_name}_submission.csv\")"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1640501315245,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"TH1beo327H2R","outputId":"fcb4ce83-cb69-40ea-f3a8-dff8c25d21b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["comments_to_score.csv  sample_submission.csv  validation_data.csv\n"]}],"source":["!ls \"/content/input/jigsaw-toxic-severity-rating/\""]},{"cell_type":"markdown","metadata":{"id":"gy9stRg5ftN1"},"source":["## validation"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":417,"status":"ok","timestamp":1640501861660,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"mbz_eSUhLiab"},"outputs":[],"source":["val_df = pd.read_csv(\"/content/input/jigsaw-toxic-severity-rating/validation_data.csv\")"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":478,"status":"ok","timestamp":1640501866386,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"7Vn7pgJehO5N"},"outputs":[],"source":["class ValJigsawDataset(Dataset):\n","    def __init__(self, df, tokenizer, max_length):\n","        self.df = df\n","        self.max_len = max_length\n","        self.tokenizer = tokenizer\n","        self.more_toxic = df['more_toxic'].values\n","        self.less_toxic = df['less_toxic'].values\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        more_toxic = self.more_toxic[index]\n","        less_toxic = self.less_toxic[index]\n","        inputs_more_toxic = self.tokenizer.encode_plus(\n","                                more_toxic,\n","                                truncation=True,\n","                                add_special_tokens=True,\n","                                max_length=self.max_len,\n","                                padding='max_length'\n","                            )\n","        inputs_less_toxic = self.tokenizer.encode_plus(\n","                                less_toxic,\n","                                truncation=True,\n","                                add_special_tokens=True,\n","                                max_length=self.max_len,\n","                                padding='max_length'\n","                            )\n","        target = 1\n","        \n","        more_toxic_ids = inputs_more_toxic['input_ids']\n","        more_toxic_mask = inputs_more_toxic['attention_mask']\n","        \n","        less_toxic_ids = inputs_less_toxic['input_ids']\n","        less_toxic_mask = inputs_less_toxic['attention_mask']\n","        \n","        return {\n","            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n","            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n","            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n","            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n","            'target': torch.tensor(target, dtype=torch.long)\n","        }\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":411,"status":"ok","timestamp":1640501875077,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"a3mLXaNzf9fE"},"outputs":[],"source":["test_dataset = ValJigsawDataset(val_df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n","test_loader = DataLoader(test_dataset, batch_size=CONFIG['train_batch_size'],\n","                         num_workers=2, shuffle=False, pin_memory=True)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1640502415413,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"Y1A6cI-eirRz"},"outputs":[],"source":["@torch.no_grad()\n","def valid_fn(model, dataloader, device):\n","    model.eval()\n","    \n","    dataset_size = 0\n","    running_loss = 0.0\n","    \n","    PREDS_less = []\n","    PREDS_more = []\n","    \n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for step, data in bar:\n","        less_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n","        less_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n","\n","        more_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n","        more_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n","\n","        less_outputs = model(less_ids, less_mask)\n","        PREDS_less.append(less_outputs.view(-1).cpu().detach().numpy()) \n","        \n","        more_outputs = model(more_ids, more_mask)\n","        PREDS_more.append(more_outputs.view(-1).cpu().detach().numpy()) \n","\n","    PREDS_less = np.concatenate(PREDS_less)\n","    PREDS_more = np.concatenate(PREDS_more)\n","    gc.collect()\n","    \n","    return PREDS_less,PREDS_more"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208956,"status":"ok","timestamp":1640503746962,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"itqAnAhRhX7M","outputId":"f5601be2-96ac-4229-88d5-c421fe46c285"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":[" @@@@@@@@@@@@@@@@@@ fold0 @@@@@@@@@@@@@@@@@@\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 941/941 [03:58<00:00,  3.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":[" @@@@@@@@@@@@@@@@@@ fold1 @@@@@@@@@@@@@@@@@@\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 941/941 [03:59<00:00,  3.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":[" @@@@@@@@@@@@@@@@@@ fold2 @@@@@@@@@@@@@@@@@@\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 3\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 941/941 [03:59<00:00,  3.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":[" @@@@@@@@@@@@@@@@@@ fold3 @@@@@@@@@@@@@@@@@@\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 4\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 941/941 [03:58<00:00,  3.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":[" @@@@@@@@@@@@@@@@@@ fold4 @@@@@@@@@@@@@@@@@@\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/input/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Getting predictions for model 5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 941/941 [03:59<00:00,  3.94it/s]\n"]}],"source":["final_preds_less = []\n","final_preds_more = []\n","jigsawamodel = JigsawModel(CONFIG['model_name']) \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","checkpoint = glob(\"../models/roberta-base/**/*\")\n","model_paths = [x for x in checkpoint if CONFIG[\"exp_name\"] in x]\n","\n","for i, path in enumerate(model_paths):\n","    print(f\" @@@@@@@@@@@@@@@@@@ fold{i} @@@@@@@@@@@@@@@@@@\")\n","    model = jigsawamodel.load_from_checkpoint(checkpoint_path=path, model_name=CONFIG['model_name'])\n","    model.to(device)\n","\n","    print(f\"Getting predictions for model {i+1}\")\n","    preds_less, preds_more = valid_fn(model, test_loader, device)\n","    final_preds_less.append(preds_less)\n","    final_preds_more.append(preds_more)\n","\n","final_preds_less = np.array(final_preds_less)\n","final_preds_less_mean = np.mean(final_preds_less, axis=0)\n","\n","final_preds_more = np.array(final_preds_more)\n","final_preds_more_mean = np.mean(final_preds_more, axis=0)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1333,"status":"ok","timestamp":1640503778619,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"DcFxQuhxhmyW"},"outputs":[],"source":["exp_name = CONFIG[\"exp_name\"]\n","\n","for i, (less, more) in enumerate(zip(final_preds_less, final_preds_more)):\n","    less_colomn = \"pred_less_toxic\"+\"fold\"+str(i)\n","    more_colomn = \"pred_more_toxic\"+\"fold\"+ str(i)\n","    val_df[less_colomn] = less\n","    val_df[more_colomn] = more\n","\n","val_df.to_csv(f\"../outputs/{exp_name}.csv\", index=False)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":908},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1640503781937,"user":{"displayName":"Chihiro Nakayama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15308812598496840598"},"user_tz":-540},"id":"bTOT8s3Fm1O1","outputId":"03bfd598-9b0d-4725-c490-f7b16e46be11"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-d52392fa-e508-4925-a6f8-7b694d6e36c9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>worker</th>\n","      <th>less_toxic</th>\n","      <th>more_toxic</th>\n","      <th>pred_less_toxicfold0</th>\n","      <th>pred_more_toxicfold0</th>\n","      <th>pred_less_toxicfold1</th>\n","      <th>pred_more_toxicfold1</th>\n","      <th>pred_less_toxicfold2</th>\n","      <th>pred_more_toxicfold2</th>\n","      <th>pred_less_toxicfold3</th>\n","      <th>pred_more_toxicfold3</th>\n","      <th>pred_less_toxicfold4</th>\n","      <th>pred_more_toxicfold4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>313</td>\n","      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n","      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n","      <td>-0.332444</td>\n","      <td>-0.224163</td>\n","      <td>-0.077043</td>\n","      <td>0.162932</td>\n","      <td>0.005236</td>\n","      <td>0.126551</td>\n","      <td>0.289431</td>\n","      <td>0.436869</td>\n","      <td>-0.390523</td>\n","      <td>-0.002944</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>188</td>\n","      <td>\"And yes, people should recognize that but the...</td>\n","      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n","      <td>-0.467329</td>\n","      <td>-0.054671</td>\n","      <td>-0.301800</td>\n","      <td>0.001471</td>\n","      <td>-0.304686</td>\n","      <td>0.261400</td>\n","      <td>-0.224889</td>\n","      <td>0.337132</td>\n","      <td>-0.473689</td>\n","      <td>0.078186</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>82</td>\n","      <td>Western Media?\\n\\nYup, because every crime in...</td>\n","      <td>\"Atom you don't believe actual photos of mastu...</td>\n","      <td>-0.553231</td>\n","      <td>-0.062983</td>\n","      <td>-0.327746</td>\n","      <td>-0.100070</td>\n","      <td>-0.125925</td>\n","      <td>0.137143</td>\n","      <td>0.211031</td>\n","      <td>0.329634</td>\n","      <td>-0.309552</td>\n","      <td>-0.051786</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>347</td>\n","      <td>And you removed it! You numbskull! I don't car...</td>\n","      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n","      <td>0.019429</td>\n","      <td>0.585690</td>\n","      <td>0.154760</td>\n","      <td>0.462644</td>\n","      <td>0.217152</td>\n","      <td>0.808583</td>\n","      <td>0.456578</td>\n","      <td>0.760138</td>\n","      <td>0.027323</td>\n","      <td>0.326627</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>539</td>\n","      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n","      <td>hey \\n\\nway to support nazis, you racist</td>\n","      <td>-0.026947</td>\n","      <td>0.093572</td>\n","      <td>-0.096769</td>\n","      <td>-0.038192</td>\n","      <td>0.265181</td>\n","      <td>0.794434</td>\n","      <td>0.393926</td>\n","      <td>0.602004</td>\n","      <td>0.093472</td>\n","      <td>0.228936</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>30103</th>\n","      <td>461</td>\n","      <td>I'm sorry. I'm not an admin. I will give you t...</td>\n","      <td>get out my large penis</td>\n","      <td>-0.584017</td>\n","      <td>0.381515</td>\n","      <td>-0.310198</td>\n","      <td>0.415605</td>\n","      <td>-0.180809</td>\n","      <td>0.808160</td>\n","      <td>0.168984</td>\n","      <td>0.916185</td>\n","      <td>-0.506556</td>\n","      <td>0.293530</td>\n","    </tr>\n","    <tr>\n","      <th>30104</th>\n","      <td>527</td>\n","      <td>I'm sorry. I'm not an admin. I will give you t...</td>\n","      <td>get out my large penis</td>\n","      <td>-0.584017</td>\n","      <td>0.381515</td>\n","      <td>-0.310198</td>\n","      <td>0.415605</td>\n","      <td>-0.180809</td>\n","      <td>0.808160</td>\n","      <td>0.168984</td>\n","      <td>0.916185</td>\n","      <td>-0.506556</td>\n","      <td>0.293530</td>\n","    </tr>\n","    <tr>\n","      <th>30105</th>\n","      <td>352</td>\n","      <td>wow...\\nare you out of your mind, how was my e...</td>\n","      <td>Piss off you slant eyed-gook</td>\n","      <td>-0.644852</td>\n","      <td>0.110157</td>\n","      <td>-0.489393</td>\n","      <td>0.061433</td>\n","      <td>-0.363664</td>\n","      <td>0.515793</td>\n","      <td>-0.321488</td>\n","      <td>0.466005</td>\n","      <td>-0.619558</td>\n","      <td>0.054510</td>\n","    </tr>\n","    <tr>\n","      <th>30106</th>\n","      <td>311</td>\n","      <td>wow...\\nare you out of your mind, how was my e...</td>\n","      <td>Piss off you slant eyed-gook</td>\n","      <td>-0.644852</td>\n","      <td>0.110157</td>\n","      <td>-0.489393</td>\n","      <td>0.061433</td>\n","      <td>-0.363664</td>\n","      <td>0.515793</td>\n","      <td>-0.321488</td>\n","      <td>0.466005</td>\n","      <td>-0.619558</td>\n","      <td>0.054510</td>\n","    </tr>\n","    <tr>\n","      <th>30107</th>\n","      <td>54</td>\n","      <td>wow...\\nare you out of your mind, how was my e...</td>\n","      <td>Piss off you slant eyed-gook</td>\n","      <td>-0.644852</td>\n","      <td>0.110157</td>\n","      <td>-0.489393</td>\n","      <td>0.061433</td>\n","      <td>-0.363664</td>\n","      <td>0.515793</td>\n","      <td>-0.321488</td>\n","      <td>0.466005</td>\n","      <td>-0.619558</td>\n","      <td>0.054510</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>30108 rows × 13 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d52392fa-e508-4925-a6f8-7b694d6e36c9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d52392fa-e508-4925-a6f8-7b694d6e36c9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d52392fa-e508-4925-a6f8-7b694d6e36c9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["       worker  ... pred_more_toxicfold4\n","0         313  ...            -0.002944\n","1         188  ...             0.078186\n","2          82  ...            -0.051786\n","3         347  ...             0.326627\n","4         539  ...             0.228936\n","...       ...  ...                  ...\n","30103     461  ...             0.293530\n","30104     527  ...             0.293530\n","30105     352  ...             0.054510\n","30106     311  ...             0.054510\n","30107      54  ...             0.054510\n","\n","[30108 rows x 13 columns]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQzZC2nKpSdr"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"inference_colab.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}
